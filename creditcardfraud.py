# -*- coding: utf-8 -*-
"""CreditCardFraud_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zwil9Ixumc_Lspv9q1CXkT-xrboqyOBE
"""

import warnings
warnings.filterwarnings("ignore")

"""Importing libraries"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df_trn = pd.read_csv("train_transaction.csv",low_memory=False)
df_idn = pd.read_csv("train_identity.csv",low_memory=False)
df = pd.merge(df_trn, df_idn,on='TransactionID', how='left')
# df_test_trn = pd.read_csv("test_transaction.csv",low_memory=False)
# df_test_idn = pd.read_csv("test_identity.csv",low_memory=False)

def data_analysis(df):  
  print('########################################################################',end="\n")
  print('First 5 listings of dataframe',end="\n\n")
  print(df.head(5))    
  print('Dataframe info',end="\n\n")
  print(df.info())
  print('Describe dataframe',end="\n\n")
  print(df.describe())
  print('All Columns in the dataframe',end="\n\n")
  print(df.columns)
  print('Datatypes of different columns in dataframe',end="\n\n")
  print(df.dtypes)
  print('Count of Null values in the dataframe',end="\n\n")
  print(df.isnull().sum())
  print('Count of NA values in the dataframe',end="\n\n")
  print(df.isna().sum())
  print('Shape of dataframe',end="\n\n")
  print(df.shape)
  print("\n")

data_analysis(df_trn)

data_analysis(df_idn)

null_variables = df_trn.isnull().sum()/len(df_trn) * 100
null_variables = null_variables.drop(null_variables[null_variables == 0].index).sort_values(ascending=False)[:500]
plt.subplots(figsize=(40,10))
plt.xticks(rotation='90')
sns.barplot(null_variables.index, null_variables)
plt.xlabel('Features', fontsize=20)
plt.ylabel('Missing rate', fontsize=20);

plt.subplots(figsize=(15,5))
plt.hist(df_trn['TransactionDT'], label='train', bins=50, color="#005BBB");
plt.legend();
plt.title('Transaction dates');

print(f"Train TransactionDT min : {df_trn.TransactionDT.min()}")
print(f"Train TransactionDT max : {df_trn.TransactionDT.max()}")

df_trn['hour'] = (df_trn['TransactionDT']//(3600))%24

train_hour = (df_trn.groupby(['isFraud'])['hour']
                     .value_counts(normalize=True)
                     .rename('percentage')
                     .mul(100)
                     .reset_index()
                     .sort_values('hour'))

plt.subplots(figsize=(10,4))
sns.barplot(x="hour", y="percentage", hue="isFraud", data=train_hour, palette=["#FFD500", "#005BBB"]);

fig, ax = plt.subplots(1, 2, figsize=(18,4))

time_val = df_trn['TransactionAmt'].values

sns.distplot(time_val, ax=ax[0], color='#FFD500')
ax[0].set_title('Distribution of TransactionAmt', fontsize=14)
ax[1].set_xlim([min(time_val), max(time_val)])

sns.distplot(np.log(time_val), ax=ax[1], color='#005BBB')
ax[1].set_title('Distribution of LOG TransactionAmt', fontsize=14)
ax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])

plt.show()

plt.figure(figsize=(12,6))
trn_ProductCD = (df_trn.groupby(['isFraud'])['ProductCD']
                     .value_counts(normalize=True)
                     .rename('percentage')
                     .mul(100)
                     .reset_index()
                     .sort_values('ProductCD'))
sns.barplot(x="ProductCD", y="percentage", hue="isFraud", data=trn_ProductCD, palette=["#FFD500", "#005BBB"]);

mylist= list(df.isnull().sum().values)

len(mylist)

"""Separating the columns which contain null values more than 100000"""

rel_list = []
irr_list = []
for index,val in enumerate(mylist):
    if val<100000:
        rel_list.append(index)
    else :
        irr_list.append(index)

len(rel_list)

columnname =[]
for index, column in enumerate(df.columns):
    if index in rel_list:
        columnname.append(column)

df[columnname].isnull().sum().values

latestdf = df[columnname]
latestdf.shape

latestdf.drop_duplicates(subset=['TransactionID','isFraud'],keep='first',inplace=True)
latestdf.shape

imputemedian=latestdf.select_dtypes(exclude=['object']).columns
len(imputemedian)

imputemode=latestdf.select_dtypes(include=['object']).columns
len(imputemode)

len(latestdf.columns)

for index,colval in enumerate(imputemedian):
    latestdf[colval].fillna(latestdf[colval].median(),inplace=True)

for index,colval in enumerate(imputemode):
    latestdf[colval].fillna(latestdf[colval].mode()[0],inplace=True)

latestdf.isnull().sum().values

latestdf.sort_values(by='TransactionID')

latestdf.isFraud.value_counts()

sns.countplot(data=latestdf,x='isFraud')

len(latestdf.P_emaildomain.value_counts())

"""Dropping the P_emaildomain column as encoding this will include 58 more columns"""

y_train = latestdf.isFraud
x_train = latestdf.drop(['isFraud','P_emaildomain'], axis =1)

latestdf.select_dtypes('object')
latestdf.ProductCD.value_counts()
latestdf.card4.value_counts()
latestdf.card6.value_counts()

# del df
# del latestdf
# del df_trn 
# del df_idn
# import gc
# gc.collect()

from imblearn.over_sampling import RandomOverSampler
over_sampler = RandomOverSampler(random_state=42)
x_res, y_res = over_sampler.fit_resample(x_train, y_train)

print(y_res.value_counts())
print(x_res.shape)
print(y_res.shape)

"""x_res.drop_duplicates(inplace=False).shape"""

sns.countplot(x=y_res)

from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(drop='first',sparse=False)

# print(x_res.ProductCD.value_counts())
# print(x_res.card4.value_counts())
# print(x_res.card6.value_counts())

ohe.fit(x_res[['ProductCD', 'card4', 'card6']])
x_encoded=ohe.transform(x_res[['ProductCD', 'card4', 'card6']])
import joblib
joblib.dump(ohe,'ohe.save')

x_enc=pd.DataFrame(data=x_encoded,columns=['ProductCD_W','ProductCD_C','ProductCD_R','ProductCD_H','card40','card41','card42','card60','card61','card62'])
x_enc

x_res.drop(['ProductCD', 'card4', 'card6'],inplace=True,axis=1)
print(x_enc.shape)
print(x_res.shape)

newres= pd.concat([x_res,x_enc],axis=1)
print(newres.shape)

del x_res
del x_enc
gc.collect()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import classification_report,accuracy_score,precision_score,confusion_matrix,f1_score,recall_score
import pickle

x_train_, x_test_, y_train_, y_test_ = train_test_split(newres,y_res,stratify=y_res, test_size=0.20, random_state=42)

def model_evaluation(modelname,model_):
  print("Training model :"+modelname+"\n\n")
  model = model_
  model.fit(x_train_,y_train_)
  predict = model.predict(x_test_)
  #print("Classification report for model "+modelname+ " is:" +str(classification_report(y_test_,predict)))
  print("Accuracy score for model "+modelname+" is:   "+str(accuracy_score(y_test_,predict)))
  print("Precision score for model "+modelname+" is : "+str(precision_score(y_test_,predict)))   
  print("F1 score for model "+modelname+" is : "+str(f1_score(y_test_,predict))) 
  print("Confusion matrix for model "+modelname+" is : "+str(confusion_matrix(y_test_,predict))) 
  print("Recall score for model "+modelname+" is : "+str(recall_score(y_test_,predict))) 
  print(model.get_params())
    
  pickle.dump(model,open('model_'+modelname+'.pkl','wb'))

import joblib
import os
from sklearn.metrics import roc_curve,auc
models = {
          'RandomForestClassifier':RandomForestClassifier(),
          'DecisionTreeClassifier':DecisionTreeClassifier(),
          'LogisticRegression':LogisticRegression(),
          'AdaBoostClassifier':AdaBoostClassifier()
}

for modelname,model in models.items() : 
    model_evaluation(modelname,model)
for modelname,model in models.items() :    
    model=joblib.load(os.path.join(os.getcwd(),'model_'+str(modelname)+'.pkl'))
    pred=model.predict(x_test_)    
    fpr, tpr, thresholds = roc_curve(y_test_, pred)    

    plt.plot(fpr, tpr)#, label='ROC curve (area = %.3f)'%auc)
    plt.legend()
    plt.title('ROC curve for '+str(modelname))
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.grid(True)
    plt.show()

from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score

random_search = {'max_depth': [5,10,20,40],               
               'min_samples_leaf': [4, 8, 12],
               'max_depth': [int(val) for val in np.linspace(10, 50, num = 5,dtype=int)],
               'min_samples_split': [5, 10, 15],
               'n_estimators': [int(val) for val in np.linspace(start = 200, stop = 2000, num = 10,dtype=int)]}

model = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_search, 
                               cv = 5, verbose= 5, random_state= 42, n_jobs = -1)
model_evaluation('hyperparametrised_rf',model)

